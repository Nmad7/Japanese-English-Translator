{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7EngJapFinalApproach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2ch9WfT1Fi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "56351b78-041b-451a-b3b4-febbc2f0de02"
      },
      "source": [
        "import os\n",
        "import sys, subprocess\n",
        "import tensorflow.keras\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, Activation, Concatenate, TimeDistributed, RepeatVector, Bidirectional, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "import gc\n",
        "import pickle\n",
        "!pip install nagisa\n",
        "import nagisa\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !ln -s \"/content/drive/My Drive/Projects/\" /Projects"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nagisa in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nagisa) (1.18.5)\n",
            "Requirement already satisfied: DyNet in /usr/local/lib/python3.6/dist-packages (from nagisa) (2.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from DyNet->nagisa) (0.29.21)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/Projects/Projects': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhxo5W11y98",
        "colab_type": "text"
      },
      "source": [
        "Load and clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBCqNuY1116U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "081e5596-d4e6-4634-ec26-a94d87363109"
      },
      "source": [
        "# start with the first 90873\n",
        "num_example = 90873\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    with open('/Projects/combo2.txt', 'r', encoding='UTF-8') as f:\n",
        "      # get each phrase in both languages\n",
        "      lines = f.read().strip().split('\\n')\n",
        "      # seperate the phrase in both langauges\n",
        "      word_pairs = [[phrase for phrase in line.split('\\t')]  for line in lines[:num_example]]\n",
        "      return zip(*word_pairs)\n",
        "\n",
        "\n",
        "en, ja = create_lang_list(num_example)\n",
        "\n",
        "ja_sentence = list()\n",
        "# remove random spaces from the japanese text\n",
        "for i in ja:\n",
        "    ja_sentence.append(i.replace(\" \", \"\"))\n",
        "\n",
        "# space Japanese text\n",
        "ja_text = list()\n",
        "for text in ja_sentence:\n",
        "    words = nagisa.tagging(text)\n",
        "    ja_text.append(\" \".join(words.words).strip())\n",
        "\n",
        "# Remove accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))\n",
        "\n",
        "def replace_special_character_to_space_en(text):\n",
        "    # replaces multiple special char in a row with just one\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    # replaces multiple spaces in row with just one\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    # replaces special char with space\n",
        "    # text = re.sub(r\"[^a-zA-Z?.!,']+\", \" \", text)\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    return text\n",
        "  \n",
        "def replace_special_character_to_space(text):\n",
        "    # replaces multiple special char in a row with just one\n",
        "    text = re.sub(r\"([?!。、¿])\", r\" \\1\", text)\n",
        "    # list of special japanese characters\n",
        "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\s、。.,0-9]+\"\n",
        "    # remove special characters\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    # remove mulitple spaces\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def normalize(english_text, japanese_text):\n",
        "    \n",
        "    input_value = []\n",
        "    target_value = []\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "        en_text = \"startl \" + en_text + \" endl\"\n",
        "        target_value.append(en_text)\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "\n",
        "        # add start and end sentence for lstm\n",
        "        # ja_text = \"startl \" + ja_text + \" endl\"\n",
        "        \n",
        "        input_value.append(ja_text)\n",
        "\n",
        "    return input_value, target_value\n",
        "\n",
        "# get normalize text data\n",
        "input_value, target_value = normalize(en, ja_text)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)\n",
        "\n",
        "X_train, X_test, \\\n",
        "    Y_train, Y_test = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
        "\n",
        "print(pd.DataFrame({\"input\": X_train, \"target\": Y_train}).head(15))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       input              target\n",
            "0       行け 。    startl go . endl\n",
            "1   行き なさい 。    startl go . endl\n",
            "2    こんにちは 。    startl hi . endl\n",
            "3     もしもし 。    startl hi . endl\n",
            "4    やっ ほー 。    startl hi . endl\n",
            "5    こんにちは !    startl hi . endl\n",
            "6       走れ 。   startl run . endl\n",
            "7     走っ て !   startl run . endl\n",
            "8        誰 ?   startl who ? endl\n",
            "9     すごい !   startl wow ! endl\n",
            "10      ワォ !   startl wow ! endl\n",
            "11      わぉ !   startl wow ! endl\n",
            "12      おー !   startl wow ! endl\n",
            "13   火事 だ !  startl fire ! endl\n",
            "14      火事 !  startl fire ! endl\n",
            "                                 input                                             target\n",
            "18680            食物 は 生きる ため に 必要 だ 。            startl food is essential to life . endl\n",
            "29020      彼 は 彼女 に 自分 の 計画 を 知ら せ た 。      startl he acquainted her with his plan . endl\n",
            "86442  あなた に 私 の 家族 の 写真 を 送る つもり だ 。  startl i am sending you a picture of my family...\n",
            "16694                   これ 使っ て も いい ?             startl am i allowed to use this ? endl\n",
            "49655                   生き て て よかっ た !                         startl this is life ! endl\n",
            "75637      その 質問 は いまだ に 頭 から 離れ ない 。        startl that question still sticks me . endl\n",
            "88410            この 辞書 は 私 の 姉 の です 。       startl this dictionary is my sister's . endl\n",
            "3520                         冗談 だ よ 。                       startl this is a joke . endl\n",
            "26165              私 も 同じ こと 考え て た 。        startl i was thinking the same thing . endl\n",
            "67981     2人 の 子供 が 通り で 遊ん で い た 。  startl there were two children playing on the ...\n",
            "61145                  ああ 、 今 思い出し た 。             startl well , i just remembered . endl\n",
            "50786               この 薬 は 僕 に 合わ ない 。  startl this medicine does not agree with me . ...\n",
            "26320     それ は 典型 的 な ゴシック 式 教会 です 。        startl it is a typical gothic church . endl\n",
            "38631         何 か 言い たい こと が あっ た の ?  startl was there something you wanted to say ?...\n",
            "68346           兄 は いつ か 帰っ て くる かしら 。  startl i wonder if my brother will ever come b...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiPnc49n6GYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "066bab8a-874a-4ebc-baa8-d24b6f846926"
      },
      "source": [
        "# empty lists\n",
        "eng_len = []\n",
        "ja_len = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in x:\n",
        "      ja_len.append(len(i.split()))\n",
        "\n",
        "for i in y:\n",
        "      eng_len.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'eng':eng_len, 'ja':ja_len})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VJP6pyF6_U1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "239223c0-8943-4d5c-8a6f-eb84617b534e"
      },
      "source": [
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "\n",
        "# prepare Japanese tokenizer\n",
        "ja_tokenizer = tokenization(x)\n",
        "with open('ja_tokenizer3.pickle', 'wb') as handle:\n",
        "    pickle.dump(ja_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "ja_vocab_size = len(ja_tokenizer.word_index) + 1\n",
        "\n",
        "ja_max_length = max(ja_len)\n",
        "print('Japanese max sentence length %d' % ja_max_length)\n",
        "print('Japanese Vocabulary Size: %d' % ja_vocab_size)\n",
        "\n",
        "# prepare English tokenizer\n",
        "eng_tokenizer = tokenization(y)\n",
        "with open('eng_tokenizer3..pickle', 'wb') as handle:\n",
        "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_max_length = max(eng_len)\n",
        "print('English max sentence length: %d' % eng_max_length)\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Japanese max sentence length 22\n",
            "Japanese Vocabulary Size: 14893\n",
            "English max sentence length: 17\n",
            "English Vocabulary Size: 9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGVAwhEQ9bVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "84e203c6-bda7-4554-89ce-785692cf471a"
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "         # integer encode sequences\n",
        "         seq = tokenizer.texts_to_sequences(lines)\n",
        "         # pad sequences with 0 values\n",
        "         seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "         return seq\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ja_tokenizer, ja_max_length, X_train)\n",
        "trainY = encode_sequences(eng_tokenizer, eng_max_length, Y_train)\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ja_tokenizer, ja_max_length, X_test)\n",
        "testY = encode_sequences(eng_tokenizer, eng_max_length, Y_test)\n",
        "\n",
        "trainX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2166,    2, 1459, ...,    0,    0,    0],\n",
              "       [   9,    2,   19, ...,    0,    0,    0],\n",
              "       [  33,    3,   10, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  10,    6, 1629, ...,    0,    0,    0],\n",
              "       [  19,    2,   36, ...,    0,    0,    0],\n",
              "       [2568,    5,   82, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOw4rMORg0c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "86fc6a23-4528-4243-cbe5-34a8c7b8458e"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(ja_tokenizer, trainX[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(eng_tokenizer, trainY[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "2166 ----> 食物\n",
            "2 ----> は\n",
            "1459 ----> 生きる\n",
            "154 ----> ため\n",
            "3 ----> に\n",
            "143 ----> 必要\n",
            "16 ----> だ\n",
            "1 ----> 。\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> startl\n",
            "350 ----> food\n",
            "7 ----> is\n",
            "2990 ----> essential\n",
            "5 ----> to\n",
            "210 ----> life\n",
            "2 ----> endl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxGg8Hc-u9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build NMT model\n",
        "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "      # model.add(Bidirectional(LSTM(512,return_sequences=False, dropout=.2)))\n",
        "      model.add(Bidirectional(GRU(1024,return_sequences=False)))\n",
        "      model.add(RepeatVector(out_timesteps))\n",
        "      # model.add(Bidirectional(LSTM(512,return_sequences=True, dropout=.2)))\n",
        "      model.add(Bidirectional(GRU(1024,return_sequences=True)))\n",
        "      model.add(TimeDistributed(Dense(out_vocab,activation='softmax')))\n",
        "      return model\n",
        "\n",
        "# model compilation\n",
        "model = define_model(ja_vocab_size, eng_vocab_size, ja_max_length, eng_max_length, 1024)\n",
        "learning_rate = 0.002\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', \n",
        "                 optimizer = optimizers.Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "\n",
        "filename = '/content/sample_data/cp_finalApproach-{epoch:02d}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3p6UzYhXXXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "a4ad1d8c-6b7a-417b-f0b3-ee1e6ae74f62"
      },
      "source": [
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=20, batch_size=128, validation_split = 0.2,callbacks=[checkpoint], \n",
        "                    verbose=1)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 2.4283 - accuracy: 0.6293\n",
            "Epoch 00001: val_loss improved from inf to 2.10537, saving model to /content/sample_data/cp_finalApproach-01.hdf5\n",
            "455/455 [==============================] - 116s 256ms/step - loss: 2.4283 - accuracy: 0.6293 - val_loss: 2.1054 - val_accuracy: 0.6507\n",
            "Epoch 2/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.9066 - accuracy: 0.6622\n",
            "Epoch 00002: val_loss improved from 2.10537 to 1.78488, saving model to /content/sample_data/cp_finalApproach-02.hdf5\n",
            "455/455 [==============================] - 114s 251ms/step - loss: 1.9066 - accuracy: 0.6622 - val_loss: 1.7849 - val_accuracy: 0.6725\n",
            "Epoch 3/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.5816 - accuracy: 0.6855\n",
            "Epoch 00003: val_loss improved from 1.78488 to 1.59406, saving model to /content/sample_data/cp_finalApproach-03.hdf5\n",
            "455/455 [==============================] - 114s 251ms/step - loss: 1.5816 - accuracy: 0.6855 - val_loss: 1.5941 - val_accuracy: 0.6880\n",
            "Epoch 4/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.3547 - accuracy: 0.7059\n",
            "Epoch 00004: val_loss improved from 1.59406 to 1.52613, saving model to /content/sample_data/cp_finalApproach-04.hdf5\n",
            "455/455 [==============================] - 114s 251ms/step - loss: 1.3547 - accuracy: 0.7059 - val_loss: 1.5261 - val_accuracy: 0.6938\n",
            "Epoch 5/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.2034 - accuracy: 0.7213\n",
            "Epoch 00005: val_loss improved from 1.52613 to 1.48961, saving model to /content/sample_data/cp_finalApproach-05.hdf5\n",
            "455/455 [==============================] - 114s 250ms/step - loss: 1.2034 - accuracy: 0.7213 - val_loss: 1.4896 - val_accuracy: 0.6975\n",
            "Epoch 6/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.0914 - accuracy: 0.7357\n",
            "Epoch 00006: val_loss improved from 1.48961 to 1.48935, saving model to /content/sample_data/cp_finalApproach-06.hdf5\n",
            "455/455 [==============================] - 114s 251ms/step - loss: 1.0914 - accuracy: 0.7357 - val_loss: 1.4894 - val_accuracy: 0.7023\n",
            "Epoch 7/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.7472\n",
            "Epoch 00007: val_loss improved from 1.48935 to 1.48847, saving model to /content/sample_data/cp_finalApproach-07.hdf5\n",
            "455/455 [==============================] - 114s 251ms/step - loss: 1.0104 - accuracy: 0.7472 - val_loss: 1.4885 - val_accuracy: 0.7040\n",
            "Epoch 8/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 0.9492 - accuracy: 0.7572\n",
            "Epoch 00008: val_loss did not improve from 1.48847\n",
            "455/455 [==============================] - 111s 244ms/step - loss: 0.9492 - accuracy: 0.7572 - val_loss: 1.5097 - val_accuracy: 0.7049\n",
            "Epoch 9/20\n",
            "455/455 [==============================] - ETA: 0s - loss: 0.8983 - accuracy: 0.7662\n",
            "Epoch 00009: val_loss did not improve from 1.48847\n",
            "455/455 [==============================] - 111s 244ms/step - loss: 0.8983 - accuracy: 0.7662 - val_loss: 1.5277 - val_accuracy: 0.7043\n",
            "Epoch 10/20\n",
            "117/455 [======>.......................] - ETA: 1:15 - loss: 0.8099 - accuracy: 0.7837"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-OSyqTdaLMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('/content/sample_data/cp_finalApproach-14.hdf5')\n",
        "\n",
        "testX = testX[:100]\n",
        "\n",
        "#trainXsamp = trainX[:100]\n",
        "\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n",
        "\n",
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j], eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))\n",
        "\n",
        "pred_df = pd.DataFrame({'actual' : Y_test[:100], 'predicted' : preds_text})\n",
        "pred_df.sample(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtXQ2BmiDw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text = \"あなたは誰ですか。\"\n",
        "text = \"あなたの背が高い\"\n",
        "# text = \"あなたの名前は何ですか。\"\n",
        "# text = \"彼ならできるでしょうが 。\"\n",
        "# text = \"日本料理が好きになった。\"\n",
        "\n",
        "spaced_text = \" \".join(word for word in nagisa.tagging(text).words)\n",
        "spaced_text = \"startl \" + spaced_text + \" endl\"\n",
        "spaced_text = ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', spaced_text))\n",
        "\n",
        "seq_text = ja_tokenizer.texts_to_sequences([spaced_text])\n",
        "# pad sequences with 0 values\n",
        "seq_text = pad_sequences(seq_text, maxlen=40, padding='post')\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(ja_tokenizer, seq_text[0])\n",
        "\n",
        "preds = model.predict_classes(seq_text)\n",
        "\n",
        "def get_word(n, tokenizer):\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "          if index == n:\n",
        "              return word\n",
        "      return None\n",
        "\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = get_word(i[j], eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))\n",
        "\n",
        "print('actual: ' + text + ' predicted: ' + preds_text[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGqjSlgopJIf",
        "colab_type": "text"
      },
      "source": [
        "Data Sources:\n",
        "\n",
        "http://www.edrdg.org/wiki/index.php/Tanaka_Corpus\n",
        "http://www.manythings.org/anki/\n",
        "\n",
        "\n",
        "Sources:\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/\n",
        "\n",
        "https://medium.com/voice-tech-podcast/programming-a-machine-translator-ef87fe10606f\n",
        "\n",
        "https://nthu-datalab.github.io/ml/labs/13-1_Seq2Seq-Learning_Neural-Machine-Translation/13-1_Seq2Seq-Learning_Neural-Machine-Translation.html\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f\n",
        "\n",
        "https://github.com/atsushii/Neural-Machine-Translation-Project\n",
        "\n"
      ]
    }
  ]
}